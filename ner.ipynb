{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYvx57XSn9RzFhcLiH0fVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitorcodesalittle/dataset-supermercado/blob/master/ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4mG38GfxjiA",
        "outputId": "dc14684d-510a-4d1b-ac2a-95c7006e801f"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRPNkshHxqun",
        "outputId": "61f11388-c500-4d67-8c68-588edc9b06a7"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/vitorcodesalittle/dataset-supermercado/master/information-extraction/labels.txt\n",
        "!wget https://raw.githubusercontent.com/vitorcodesalittle/dataset-supermercado/master/information-extraction/sentences.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-03 21:14:12--  https://raw.githubusercontent.com/vitorcodesalittle/dataset-supermercado/master/information-extraction/labels.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 127 [text/plain]\n",
            "Saving to: ‘labels.txt.1’\n",
            "\n",
            "labels.txt.1        100%[===================>]     127  --.-KB/s    in 0s      \n",
            "\n",
            "2021-08-03 21:14:14 (4.82 MB/s) - ‘labels.txt.1’ saved [127/127]\n",
            "\n",
            "--2021-08-03 21:14:14--  https://raw.githubusercontent.com/vitorcodesalittle/dataset-supermercado/master/information-extraction/sentences.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 327 [text/plain]\n",
            "Saving to: ‘sentences.txt.1’\n",
            "\n",
            "sentences.txt.1     100%[===================>]     327  --.-KB/s    in 0s      \n",
            "\n",
            "2021-08-03 21:14:14 (18.7 MB/s) - ‘sentences.txt.1’ saved [327/327]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU4e0KmVxs_-",
        "outputId": "6fe1e293-da15-46fb-ddb8-8ca25cb2c957"
      },
      "source": [
        "def file2Examples(labels, sentences):  \n",
        "  examples=[]\n",
        "\n",
        "  with open(labels,\"r\") as l:\n",
        "    with open(sentences, \"r\") as s:\n",
        "      for label in l:\n",
        "        example = [[],[]]\n",
        "        sentence = s.readline()\n",
        "\n",
        "        label_split= label.split()\n",
        "        sentence_split = sentence.split()\n",
        "          \n",
        "        if len(label_split) == len(sentence_split):\n",
        "          example[0] = sentence_split\n",
        "          example[1] = label_split\n",
        "          examples.append(example)\n",
        "    \n",
        "    return examples\n",
        "\n",
        "examples = file2Examples(\"labels.txt\", \"sentences.txt\")\n",
        "print(examples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[['Coloca', 'uma', 'manteiga'], ['O', 'BQ', 'BP']], [['Tira', 'a', 'manteiga'], ['O', 'O', 'BP']], [['Adicionar', 'açucar', 'demerara', '500g'], ['O', 'BP', 'IP', 'BQ']], [['Inserir', 'peito', 'de', 'frango', 'fatiado'], ['O', 'BP', 'IP', 'IP', 'IP']], [['Colocar', 'cerveja', 'colorado', 'appia'], ['O', 'BP', 'BM', 'IP']], [['Tira', 'a', 'manteiga'], ['O', 'O', 'BP']], [['Remover', 'presunto', 'sadia'], ['O', 'BP', 'BM']], [['Tirar', 'palma', 'banana', 'prata'], ['O', 'BQ', 'BP', 'IP']], [['Excluir', 'queijo', 'do', 'reino'], ['O', 'BP', 'IP', 'IP']], [['Retirar', 'chocolate', 'diamante', 'negro'], ['O', 'BP', 'BM', 'IM']], [['Ver', 'pão', 'francês'], ['O', 'BP', 'IP']], [['Mostrar', 'detergente'], ['O', 'BP']], [['Vinho', 'branco', 'albaclara', 'sauvignon', 'blanc'], ['BP', 'IP', 'BM', 'IM', 'IM']]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8vSd0cD4Mvz",
        "outputId": "794c2eda-c3f5-4f09-fa09-a0be9aebb505"
      },
      "source": [
        "    # create character vocab\n",
        "    labels = ['O', 'BP', 'BM', 'IM', 'IP', 'BQ']\n",
        "    all_text = \" \".join([\" \".join(x[0]) for x in examples])\n",
        "    vocab = sorted(set(all_text))\n",
        "    \n",
        "    # create character/id and label/id mapping\n",
        "    char2idx = {u:i+1 for i, u in enumerate(vocab)}\n",
        "    idx2char = np.array(vocab)\n",
        "    label2idx = {u:i+1 for i, u in enumerate(labels)}\n",
        "    idx2label = np.array(labels)\n",
        "    \n",
        "    print(idx2label)\n",
        "    print(char2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O' 'BP' 'BM' 'IM' 'IP' 'BQ']\n",
            "{' ': 1, '0': 2, '5': 3, 'A': 4, 'C': 5, 'E': 6, 'I': 7, 'M': 8, 'R': 9, 'T': 10, 'V': 11, 'a': 12, 'b': 13, 'c': 14, 'd': 15, 'e': 16, 'f': 17, 'g': 18, 'h': 19, 'i': 20, 'j': 21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'x': 33, 'ã': 34, 'ç': 35, 'ê': 36}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SoMz4hW56_3",
        "outputId": "dde4f0a1-d966-46d6-ad56-98ff07adf372"
      },
      "source": [
        "def split_char_labels(eg):\n",
        "      \n",
        "      tokens = eg[0]\n",
        "      labels= eg[1]\n",
        "\n",
        "      input_chars = []\n",
        "      output_char_labels = []\n",
        "\n",
        "      for token,label in zip(tokens,labels):\n",
        "\n",
        "        input_chars.extend([char for char in token])\n",
        "        input_chars.extend(' ')\n",
        "        output_char_labels.extend([label]*len(token))\n",
        "        output_char_labels.extend('O')\n",
        "\n",
        "      return [[char2idx[x] for x in input_chars[:-1]],np.array([label2idx[x] for x in output_char_labels[:-1]])]\n",
        "   \n",
        "train_formatted = [split_char_labels(eg) for eg in examples]\n",
        "\n",
        "print(len(train_formatted))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[5, 25, 22, 25, 14, 12, 1, 31, 23, 12, 1, 23, 12, 24, 30, 16, 20, 18, 12], array([1, 1, 1, 1, 1, 1, 1, 6, 6, 6, 1, 2, 2, 2, 2, 2, 2, 2, 2])], [[10, 20, 28, 12, 1, 12, 1, 23, 12, 24, 30, 16, 20, 18, 12], array([1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2])], [[4, 15, 20, 14, 20, 25, 24, 12, 28, 1, 12, 35, 31, 14, 12, 28, 1, 15, 16, 23, 16, 28, 12, 28, 12, 1, 3, 2, 2, 18], array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 5, 5, 5, 5, 5,\n",
            "       5, 5, 5, 1, 6, 6, 6, 6])], [[7, 24, 29, 16, 28, 20, 28, 1, 26, 16, 20, 30, 25, 1, 15, 16, 1, 17, 28, 12, 24, 18, 25, 1, 17, 12, 30, 20, 12, 15, 25], array([1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 5, 5, 1, 5, 5, 5, 5, 5,\n",
            "       5, 1, 5, 5, 5, 5, 5, 5, 5])], [[5, 25, 22, 25, 14, 12, 28, 1, 14, 16, 28, 32, 16, 21, 12, 1, 14, 25, 22, 25, 28, 12, 15, 25, 1, 12, 26, 26, 20, 12], array([1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 3, 3, 3, 3, 3, 3,\n",
            "       3, 3, 1, 5, 5, 5, 5, 5])], [[10, 20, 28, 12, 1, 12, 1, 23, 12, 24, 30, 16, 20, 18, 12], array([1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2])], [[9, 16, 23, 25, 32, 16, 28, 1, 26, 28, 16, 29, 31, 24, 30, 25, 1, 29, 12, 15, 20, 12], array([1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 3, 3, 3, 3, 3])], [[10, 20, 28, 12, 28, 1, 26, 12, 22, 23, 12, 1, 13, 12, 24, 12, 24, 12, 1, 26, 28, 12, 30, 12], array([1, 1, 1, 1, 1, 1, 6, 6, 6, 6, 6, 1, 2, 2, 2, 2, 2, 2, 1, 5, 5, 5,\n",
            "       5, 5])], [[6, 33, 14, 22, 31, 20, 28, 1, 27, 31, 16, 20, 21, 25, 1, 15, 25, 1, 28, 16, 20, 24, 25], array([1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 5, 5, 1, 5, 5, 5, 5,\n",
            "       5])], [[9, 16, 30, 20, 28, 12, 28, 1, 14, 19, 25, 14, 25, 22, 12, 30, 16, 1, 15, 20, 12, 23, 12, 24, 30, 16, 1, 24, 16, 18, 28, 25], array([1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 3, 3, 3, 3,\n",
            "       3, 3, 3, 3, 1, 4, 4, 4, 4, 4])], [[11, 16, 28, 1, 26, 34, 25, 1, 17, 28, 12, 24, 14, 36, 29], array([1, 1, 1, 1, 2, 2, 2, 1, 5, 5, 5, 5, 5, 5, 5])], [[8, 25, 29, 30, 28, 12, 28, 1, 15, 16, 30, 16, 28, 18, 16, 24, 30, 16], array([1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])], [[11, 20, 24, 19, 25, 1, 13, 28, 12, 24, 14, 25, 1, 12, 22, 13, 12, 14, 22, 12, 28, 12, 1, 29, 12, 31, 32, 20, 18, 24, 25, 24, 1, 13, 22, 12, 24, 14], array([2, 2, 2, 2, 2, 1, 5, 5, 5, 5, 5, 5, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "       1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4])]]\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIfFwzzV62Ie"
      },
      "source": [
        "    # training generator\n",
        "    def gen_train_series():\n",
        "\n",
        "        for eg in train_formatted:\n",
        "          yield eg[0],eg[1]\n",
        "      \n",
        "    # create Dataset objects for train, test and validation sets  \n",
        "    series = tf.data.Dataset.from_generator(gen_train_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
        "\n",
        "    BATCH_SIZE = 128\n",
        "    BUFFER_SIZE = 1000\n",
        "    \n",
        "    # create padded batch series objects for train, test and validation sets\n",
        "    ds_series_batch = series.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
        "    \n",
        "    # print example batches\n",
        "    for input_example_batch, target_example_batch in ds_series_batch:\n",
        "      print(input_example_batch)\n",
        "      print(target_example_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYzhxh7zNw8t",
        "outputId": "923e1c03-f5c6-4b5a-b303-4fd59cf688c0"
      },
      "source": [
        "  vocab_size = len(vocab)+1\n",
        "\n",
        "  # The embedding dimension\n",
        "  embedding_dim = 256\n",
        "\n",
        "  # Number of RNN units\n",
        "  rnn_units = 1024\n",
        "\n",
        "  label_size = len(labels)  \n",
        "  \n",
        "  # build LSTM model\n",
        "  def build_model(vocab_size,label_size, embedding_dim, rnn_units, batch_size):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None],mask_zero=True),\n",
        "            tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "            tf.keras.layers.Dense(label_size)\n",
        "            ])\n",
        "        return model\n",
        "\n",
        "  model = build_model(\n",
        "        vocab_size = len(vocab)+1,\n",
        "        label_size=len(labels)+1,\n",
        "        embedding_dim=embedding_dim,\n",
        "        rnn_units=rnn_units,\n",
        "        batch_size=BATCH_SIZE)\n",
        "\n",
        "  model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (128, None, 256)          9472      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (128, None, 1024)         5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (128, None, 7)            7175      \n",
            "=================================================================\n",
            "Total params: 5,263,623\n",
            "Trainable params: 5,263,623\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVQsZ06mN86-"
      },
      "source": [
        "    import os\n",
        "\n",
        "    # define loss function\n",
        "    def loss(labels, logits):\n",
        "        return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "    model.compile(optimizer='adam', loss=loss,metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "    # Directory where the checkpoints will be saved\n",
        "    checkpoint_dir = './training_checkpoints'\n",
        "    # Name of the checkpoint files\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "    checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_prefix,\n",
        "        save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "jzhEHnLmOCYG",
        "outputId": "a75d2b20-20db-48eb-c430-32ca47a583c7"
      },
      "source": [
        "EPOCHS=20\n",
        "  \n",
        "history = model.fit(ds_series_batch, epochs=EPOCHS,callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-17fc3600d6e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_series_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expect x to be a non-empty array or dataset.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expect x to be a non-empty array or dataset."
          ]
        }
      ]
    }
  ]
}